# Probability Theory {#prob}

- Complement probability
- MGF: $M(t)=E[e^{tX}]=\int_{-\infty}^{\infty}e^{tx}f(x)dx$
- Common distributions (expecation, variance) & MGF derivations
  - Vs. characteristic function https://en.wikipedia.org/wiki/Moment-generating_function
  - Normal distribution: PDF: $f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-(x-\mu)^2/2\sigma^2}$ MGF: $e^{ut+\frac{\sigma^2t^2}{2}}$ 
  - Chi Squared 
- Combinatorics
  - Permutation: $\frac{n!}{n_1!n_2!...n_r!}$
  - Choose formula: $\binom{n}{r} = \frac{n!}{r!(n-r)!}$
  - Binomial theorem: $(x+y)^n=\sum_{k=o}^{n}\binom{n}{k}x^ky^{n-k}$
    - proof?
  - inclusion/exclusion principle
  - Stars & bars, grid walking
- Central limit theorem
  - relate to simulation # of samples
- Law of large numbers (and its uses)
  - Strong vs. weak
  - use in rules of stoch calc (stoch models lect 21)
- Law of total probability
- Conditional probability
  - Law of total expectation (Tower rule/property), related to law of total prob
  - ex. Trapped miner example, spider on cube example
  - Used in Markov chains
  - Look for resetting/recursion
  - law of total variance (application to conditional monte carlo)
  - Law of total covariance
- Bayes formula
  - Application to Gibbs Sampler in Data Analysis
- Gambler’s ruin (also as state)
- Dice game: continuation if 4,5,6. Same principle of states like cave question
- Look for games to start over again
- Wald’s Equality
- Jensen's inequality
- cauchy inequality: cov^2 <= var x var y
- Linearity in independent variances, non-indep corr triangle
  - Diff between 0 corr and independence
  - Relation to Linear Algebra
- Variance & Covariance formulas (E[XY]-E[X]E[Y], etc.)
- Bessel correction
- Convolution - sums of indep random variables, ex triangular density
- poisson process
  - definition
  - interval time is exponentially distributed, arrival times are gamma
- error function (erf, used in applications programing var of robust arb model) https://en.wikipedia.org/wiki/Error_function
- indicator expectation - probability relation (see Foundations of FE black scholes)
- joint distribution, marginal distribution, conditional distribution
- quantile
- add ross citation
[@ross2014]

## Discrete Distributions {#prob-discrete-dist}

## Continuous Distributions {#prob-continuous-dist}

### Normal distribution {#prob-continuous-dist-normal}
bivariate, multi-variate (relate to mvo in data analysis, stoch?), cholesky decomp of covar (relate to lin alg), relation to binomial tree, sums are normal, cdf used in confidence interval (data analysis)

### Lognormal distrbution

### Uniform distribution 

### Beta distribution 

### Gamma distribution
relation to chi square, inverse gamma (data analylsis mvo)

### Exponential distribution
memoryless property application, relation to geometric, hyperexponential (pdf is sum of exponential parameters, releate to simulation)

### Bernoulli distribution
relation to binomial, bernoulli process

### Geometric distribution
hypergeometric, relation to exponential

### Binomial distribution
relation to poisson, relation to bernoulli (sum of bernouli)

### Negative binomial distribution
From stoch models, relation to gamma

### Poisson distribution
relation to binomial, poisson process

### Weibull distribution
### Cauchy distribution
### Beta distribution
relate to simulation accept/reject algo
### Chi-squared distribution 

sum of indep std normals, relation to gamma distrbution, relation to F test, likelihood ratio test

### Student-t distribution 


### F distribution

https://en.wikipedia.org/wiki/F-distribution


### Wishart distribution 

from data analysis
relation to gamma and chi square from there

### Inverse wishart distribution

## Law of large numbers {#prob-lln}

- deriving bounds
  - Markov's inequality
  - Chebyshev's inequality

### Weak {#prob-lln-weak}

```{theorem, name="The weak law of large numbers", label="weak-lln"}
Suppose $\left\{X_n\right\}_{n\ge1}$ is a sequence of independent and identically distributed random variables, each having finite mean $E\left[X_i\right]=\mu$. Then for any $\epsilon>0$, $$P\left\{\left|\frac{\sum_{i=1}^nX_i}{n}-\mu\right|\ge\epsilon\right\}\rightarrow0\ \text{as}\ n\rightarrow\infty\ $$
```

```{proof}
comes via ross. need assumption that rvs have finite variance $\sigma^2$
```

The PROBABILITY of the abs demeaned average being greater than an arbitrarily small positive constant goes to zero as n goes to infinity. 

Look up general form of this proof from Prob Theory class. it's  a convergence in prob

Difference to strong LLN is weak is a statement of the evolution of probability (average likely to be near mu for large n), whereas strong is about the evolution of the level (average bound to stay near mu for m>n)

### Strong {#prob-lln-strong}

```{theorem, name="The strong law of large numbers", label="strong-lln"}
Suppose $\left\{X_n\right\}_{n\ge1}$ is a sequence of independent and identically distributed random variables, each having finite mean $E\left[X_i\right]=\mu$. Then, with probability 1, $$\frac{\sum_{i=1}^nX_i}{n}\rightarrow\mu\ \text{as}\ n\rightarrow\infty\ $$
```



## Central limit theorem {#prob-clt}

- sum of independent RVs converge in distribution to normal distribution
- convergence in distribution
- proof from ross

applied to sums of random variables
